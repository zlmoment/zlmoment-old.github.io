<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      CUDA中并行规约（Parallel Reduction）的优化 | Hackecho 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Zhaoyu (Alan) Li">
    
    
      <meta name="description" content="Parallel Reduction是NVIDIA-CUDA自带的例子，也几乎是所有CUDA学习者的的必看算法。在这个算法的优化中，Mark Harris为我们实现了7种不同的优化版本，将Bandwidth几乎提高到了峰值。相信我们通过仔细研读这个过程，一定能对CUDA程序的优化有更加深刻的认识。下面我们来一一细看这几种优化方案，数据和思想均摘录自官方SDK中Samples的算法说明。
Paral">
    

    <meta name="description" content="Parallel Reduction是NVIDIA-CUDA自带的例子，也几乎是所有CUDA学习者的的必看算法。在这个算法的优化中，Mark Harris为我们实现了7种不同的优化版本，将Bandwidth几乎提高到了峰值。相信我们通过仔细研读这个过程，一定能对CUDA程序的优化有更加深刻的认识。下面我们来一一细看这几种优化方案，数据和思想均摘录自官方SDK中Samples的算法说明。
Paral">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA中并行规约（Parallel Reduction）的优化 | Hackecho">
<meta property="og:url" content="http://hackecho.com/2013/04/cuda-parallel-reduction/">
<meta property="og:site_name" content="Hackecho">
<meta property="og:description" content="Parallel Reduction是NVIDIA-CUDA自带的例子，也几乎是所有CUDA学习者的的必看算法。在这个算法的优化中，Mark Harris为我们实现了7种不同的优化版本，将Bandwidth几乎提高到了峰值。相信我们通过仔细研读这个过程，一定能对CUDA程序的优化有更加深刻的认识。下面我们来一一细看这几种优化方案，数据和思想均摘录自官方SDK中Samples的算法说明。
Paral">
<meta property="og:image" content="/images/2013/04/parallel_reduction_tree_based.png">
<meta property="og:image" content="/images/2013/04/recuisive_tree_based.png">
<meta property="og:image" content="/images/2013/04/base.png">
<meta property="og:image" content="/images/2013/04/Interleaved_Addressing.png">
<meta property="og:image" content="/images/2013/04/Interleaved_Addressing2.png">
<meta property="og:image" content="/images/2013/04/Sequential_Addressing.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CUDA中并行规约（Parallel Reduction）的优化 | Hackecho">
<meta name="twitter:description" content="Parallel Reduction是NVIDIA-CUDA自带的例子，也几乎是所有CUDA学习者的的必看算法。在这个算法的优化中，Mark Harris为我们实现了7种不同的优化版本，将Bandwidth几乎提高到了峰值。相信我们通过仔细研读这个过程，一定能对CUDA程序的优化有更加深刻的认识。下面我们来一一细看这几种优化方案，数据和思想均摘录自官方SDK中Samples的算法说明。
Paral">

    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css" type="text/css">


    <script src="/js/jquery.min.js"></script>

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">



  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Hackecho</a></h1>

        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">
          Blog by Zhaoyu (Alan) Li
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">Blogs</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">About</a></li>
              
                
                <li class="navigation__item"><a href="/projects" title="" class="">Projects</a></li>
              
                
                <li class="navigation__item"><a href="/leetcode" title="" class="">LeetCode</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

------------------------------ -->

<nav class="cover-navigation navigation--social">
  <ul class="navigation">


  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/zlmoment" title="Zhaoyu Li on GitHub">
      <i class='icon icon-social-github'></i>
      <span class="label">GitHub</span>
    </a>
  </li>
  <li class="navigation__item">
    <a href="http://www.linkedin.com/in/zhaoyuli/" title="Zhaoyu Li on LinkedIn">
      <i class='icon icon-social-linkedin'></i>
      <span class="label">LinkedIn</span>
    </a>
  </li>

  </ul>
</nav>

        </div>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        <p class="panel-cover__description" style="font-size:12px;">
          如果你现在是夜晚，请早点休息，愿你有一个美丽的梦。
          <br />如果你现在是白天，愿你在这崭新的一天里，给未来留下一个美好的回忆。
        </p>

      </div>

    </div>

    <div class="panel-cover--overlay cover-aqua"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

  <article class="post-container post-container--single">

    <header class="post-header">
    
      
      <div class="post-meta">
        <time datetime="Apr 24 2013" class="post-meta__date date">Apr 24 2013</time> &#8226; 

        <span class="post-meta__tags tags">

            <font class="categories">
              
            </font>
            
              <font class="tags">
                <a class="tags-link" href="/tags/cuda,parallel,reduction/">cuda,parallel,reduction</a>
              </font>
            

        </span>
      </div>
      

      <h1 class="post-title">CUDA中并行规约（Parallel Reduction）的优化</h1>
    </header>

    <section class="post">
      <p>Parallel Reduction是NVIDIA-CUDA自带的例子，也几乎是所有CUDA学习者的的必看算法。在这个算法的优化中，Mark Harris为我们实现了7种不同的优化版本，将Bandwidth几乎提高到了峰值。相信我们通过仔细研读这个过程，一定能对CUDA程序的优化有更加深刻的认识。下面我们来一一细看这几种优化方案，<strong>数据和思想均摘录自官方SDK中Samples的算法说明</strong>。</p>
<h3 id="Parallel_Reduction">Parallel Reduction</h3>
<p>Parallel Reduction可以理解为将一个数组中的所有数相加求和的过程并行化。一般来讲，我们并行化的思路是基于“树”的二元规约，如下图：</p>
<p><a href="/images/2013/04/parallel_reduction_tree_based.png"><img src="/images/2013/04/parallel_reduction_tree_based.png" alt=""></a></p>
<a id="more"></a>

<p>但是这样的算法会产生一个问题，就是我们怎样让不同blocks中的线程通信呢？CUDA本身并不支持全局同步（global synchronization）。但是，CUDA的kernel运行时有一个特性，即同一时间只能有一个kernel运行，这样我们便可以将每一层规约作为一个kernel来重复递归调用。如下图：</p>
<p><a href="/images/2013/04/recuisive_tree_based.png"><img src="/images/2013/04/recuisive_tree_based.png" alt=""></a></p>
<p>我们的目标就是基于这个算法进行优化，达到“榨干CUDA性能”的目的。我们选取Bandwidth作为测量标准（因为Bandwidth侧重于测量memory-bound kernels，而GFLOP/s侧重于测量compute-bound kernels）。我们最终目标是实现最大的Data Bandwidth。测试环境为G80 GPU，384-bit memory interface, 900 MHz DDR，Bandwidth峰值384 * 1800 / 8 = 86.4 GB/s。</p>
<p>对于基本概念，放上一张图供参考：</p>
<p><a href="/images/2013/04/base.png"><img src="/images/2013/04/base.png" alt=""></a></p>
<h3 id="Reduction_#1:_Interleaved_Addressing">Reduction #1: Interleaved Addressing</h3>
<p>Interleaved Addressing的核心思想在于交错寻址，即典型的树状模型。示意图如下：</p>
<p><a href="/images/2013/04/Interleaved_Addressing.png"><img src="/images/2013/04/Interleaved_Addressing.png" alt=""></a></p>
<a id="more"></a>


<pre><code>/* <span class="type">This</span> reduction interleaves which threads are active by <span class="keyword">using</span> the modulo
   operator.  <span class="type">This</span> operator <span class="keyword">is</span> very expensive on <span class="type">GPUs</span>, <span class="keyword">and</span> the interleaved
   inactivity means that no whole warps are active, which <span class="keyword">is</span> also very
   inefficient 
*/
<span class="keyword">template</span> &lt;class T&gt;
__global__ <span class="type">void</span>
reduce0(T *g_idata, T *g_odata, unsigned <span class="type">int</span> n)
{
    T *sdata = <span class="type">SharedMemory</span>&lt;T&gt;();

    // load <span class="literal">shared</span> mem
    unsigned <span class="type">int</span> tid = threadIdx.x;
    unsigned <span class="type">int</span> i = blockIdx.x*blockDim.x + threadIdx.x;

    sdata[tid] = (i &lt; n) ? g_idata[i] : <span class="number">0</span>;

    __syncthreads();

    // <span class="keyword">do</span> reduction <span class="keyword">in</span> <span class="literal">shared</span> mem
    <span class="keyword">for</span> (unsigned <span class="type">int</span> s=<span class="number">1</span>; s &lt; blockDim.x; s *= <span class="number">2</span>)
    {
        // modulo arithmetic <span class="keyword">is</span> slow!
        <span class="keyword">if</span> ((tid % (<span class="number">2</span>*s)) == <span class="number">0</span>)
        {
            sdata[tid] += sdata[tid + s];
        }

        __syncthreads();
    }

    // write <span class="literal">result</span> <span class="keyword">for</span> this <span class="keyword">block</span> to global mem
    <span class="keyword">if</span> (tid == <span class="number">0</span>) g_odata[blockIdx.x] = sdata[<span class="number">0</span>];
}
</code></pre><p><strong>存在的问题：</strong></p>
<p>上述代码中for循环内部，容易出现线程束的分化（Warp Divergence），即同一个Warp中的线程需要执行不同的逻辑分支（详见<a href="http://people.maths.ox.ac.uk/gilesm/cuda/lecs/lec3-2x2.pdf" target="_blank" rel="external">这里</a>），这是非常低效的，而且 <code>&amp;</code> 运算也是非常慢的。测试结果如下(4M element)：</p>
<pre><code><span class="attribute">:::BASH</span>    
<span class="header">                                    Time (2^22 ints)      Bandwidth
--------------------------------------------------------------------------</span>
Kernel 1
(interleaved addressing with        8.054 ms              2.083 GB/s
divergent branching)
</code></pre><p>注意：Block Size = 128 threads for all tests.</p>
<h3 id="Reduction_#2:_Interleaved_Addressing">Reduction #2: Interleaved Addressing</h3>
<p>为了尽量减少1中的线程束的分化，我们这一步将分化的分支替换为跨步寻址（strided index）：</p>
<pre><code>/* <span class="type">This</span> version uses contiguous threads, but its interleaved
   addressing results <span class="keyword">in</span> many <span class="literal">shared</span> memory bank conflicts.
*/
<span class="keyword">template</span> &lt;class T&gt;
__global__ <span class="type">void</span>
reduce1(T *g_idata, T *g_odata, unsigned <span class="type">int</span> n)
{
    T *sdata = <span class="type">SharedMemory</span>&lt;T&gt;();

    // load <span class="literal">shared</span> mem
    unsigned <span class="type">int</span> tid = threadIdx.x;
    unsigned <span class="type">int</span> i = blockIdx.x*blockDim.x + threadIdx.x;

    sdata[tid] = (i &lt; n) ? g_idata[i] : <span class="number">0</span>;

    __syncthreads();

    // <span class="keyword">do</span> reduction <span class="keyword">in</span> <span class="literal">shared</span> mem
    <span class="keyword">for</span> (unsigned <span class="type">int</span> s=<span class="number">1</span>; s &lt; blockDim.x; s *= <span class="number">2</span>)
    {
        <span class="type">int</span> index = <span class="number">2</span> * s * tid;

        <span class="keyword">if</span> (index &lt; blockDim.x)
        {
            sdata[index] += sdata[index + s];
        }

        __syncthreads();
    }

    // write <span class="literal">result</span> <span class="keyword">for</span> this <span class="keyword">block</span> to global mem
    <span class="keyword">if</span> (tid == <span class="number">0</span>) g_odata[blockIdx.x] = sdata[<span class="number">0</span>];
}
</code></pre><p>示意图如下（注意下图与上图中Thread ID的区别）：</p>
<p><a href="/images/2013/04/Interleaved_Addressing2.png"><img src="/images/2013/04/Interleaved_Addressing2.png" alt=""></a></p>
<p>这里我们遇到一个新的问题，即Shared Memory Bank Conflicts。为了达到高带宽，Shared Memory被划分成许多大小相同的内存块，叫做Banks。Banks可以同步访问，即不同的地址对不同的Banks可以同时读写。但是，如果两个内存请求的地址落到同一个Bank上，将会导致Bank Conflicts，严重影响并行程序的性能。</p>
<p>运行结果如下(4M element)：</p>
<pre><code><span class="attribute">:::BASH</span>    
<span class="code">                                Time(2^22 ints)   Bandwidth    Step Speedup   Culmulative</span>
<span class="header">                                                                              Speedup
-----------------------------------------------------------------------------------------</span>
Kernel 1
(interleaved addressing with    8.054 ms          2.083 GB/s 
divergent branching)

Kernel 2
(interleaved addressing with    3.456 ms          4.854 GB/s   2.33x          2.33x
bank conflicts)
</code></pre><h3 id="Reduction_#3:_Sequential_Addressing">Reduction #3: Sequential Addressing</h3>
<p>我们知道，CUDA中对数据的连续读取效率要比其它方式高。因此我们这一步优化主要是将取址方式变为连续的。我们只需要将2中跨步寻址（strided index）替换为基于threadID的逆向for循环即可。</p>
<pre><code>/*
    <span class="type">This</span> version uses sequential addressing -- no divergence <span class="keyword">or</span> bank conflicts.
*/
<span class="keyword">template</span> &lt;class T&gt;
__global__ <span class="type">void</span>
reduce2(T *g_idata, T *g_odata, unsigned <span class="type">int</span> n)
{
    T *sdata = <span class="type">SharedMemory</span>&lt;T&gt;();

    // load <span class="literal">shared</span> mem
    unsigned <span class="type">int</span> tid = threadIdx.x;
    unsigned <span class="type">int</span> i = blockIdx.x*blockDim.x + threadIdx.x;

    sdata[tid] = (i &lt; n) ? g_idata[i] : <span class="number">0</span>;

    __syncthreads();

    // <span class="keyword">do</span> reduction <span class="keyword">in</span> <span class="literal">shared</span> mem
    <span class="keyword">for</span> (unsigned <span class="type">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">0</span>; s&gt;&gt;=<span class="number">1</span>)
    {
        <span class="keyword">if</span> (tid &lt; s)
        {
            sdata[tid] += sdata[tid + s];
        }

        __syncthreads();
    }

    // write <span class="literal">result</span> <span class="keyword">for</span> this <span class="keyword">block</span> to global mem
    <span class="keyword">if</span> (tid == <span class="number">0</span>) g_odata[blockIdx.x] = sdata[<span class="number">0</span>];
}
</code></pre><p>示意图如下：</p>
<p><a href="/images/2013/04/Sequential_Addressing.png"><img src="/images/2013/04/Sequential_Addressing.png" alt=""></a></p>
<p>但新的问题又出现了，我们发现在for循环中，因为 <code>if (tid &lt; s)</code> 的缘故，在第一次循环的时候有一半的线程都处于闲置状态！如果我们能全部利用的话，相信性能还会提升很多。这也是我们以后要进行优化的地方，避免线程闲置。</p>
<p>本次运行结果如下(4M element)：</p>
<pre><code><span class="attribute">:::BASH</span>    
<span class="code">                                Time(2^22 ints)   Bandwidth    Step Speedup   Culmulative </span>
<span class="header">                                                                              Speedup
-----------------------------------------------------------------------------------------</span>
Kernel 1
(interleaved addressing with    8.054 ms          2.083 GB/s 
divergent branching)

Kernel 2
(interleaved addressing with    3.456 ms          4.854 GB/s   2.33x          2.33x
bank conflicts)

Kernel 3
(sequential addressing)         1.722 ms          9.741 GB/s   2.01x          4.68x
</code></pre><h3 id="Reduction_#4:_First_Add_During_Load">Reduction #4: First Add During Load</h3>
<p>在以前的所有版本中，我们都是事先将global的数据读入共享内存 <code>sdata[tid] = (i &lt; n) ? g_idata[i] : 0;</code> ，我们可不可以在这一步进行优化呢？当然，我们这一步优化的目的是在将数据读入到共享内存时同时进行第一次(第一层)规约。</p>
<pre><code>:::C
/*
    <span class="type">This</span> version uses n/<span class="number">2</span> threads --
    it performs the first level <span class="keyword">of</span> reduction <span class="keyword">when</span> reading <span class="keyword">from</span> global memory.
*/
<span class="keyword">template</span> &lt;class T&gt;
__global__ <span class="type">void</span>
reduce3(T *g_idata, T *g_odata, unsigned <span class="type">int</span> n)
{
    T *sdata = <span class="type">SharedMemory</span>&lt;T&gt;();

    // perform first level <span class="keyword">of</span> reduction,
    // reading <span class="keyword">from</span> global memory, writing to <span class="literal">shared</span> memory
    unsigned <span class="type">int</span> tid = threadIdx.x;
    unsigned <span class="type">int</span> i = blockIdx.x*(blockDim.x*<span class="number">2</span>) + threadIdx.x;

    T mySum = (i &lt; n) ? g_idata[i] : <span class="number">0</span>;

    <span class="keyword">if</span> (i + blockDim.x &lt; n)
        mySum += g_idata[i+blockDim.x];

    sdata[tid] = mySum;
    __syncthreads();

    // <span class="keyword">do</span> reduction <span class="keyword">in</span> <span class="literal">shared</span> mem
    <span class="keyword">for</span> (unsigned <span class="type">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">0</span>; s&gt;&gt;=<span class="number">1</span>)
    {
        <span class="keyword">if</span> (tid &lt; s)
        {
            sdata[tid] = mySum = mySum + sdata[tid + s];
        }

        __syncthreads();
    }

    // write <span class="literal">result</span> <span class="keyword">for</span> this <span class="keyword">block</span> to global mem
    <span class="keyword">if</span> (tid == <span class="number">0</span>) g_odata[blockIdx.x] = sdata[<span class="number">0</span>];
}
</code></pre><p>本次运行结果如下(4M element)：</p>
<pre><code><span class="attribute">:::BASH</span>    
<span class="code">                                Time(2^22 ints)   Bandwidth    Step Speedup   Culmulative </span>
<span class="header">                                                                              Speedup
-----------------------------------------------------------------------------------------</span>
Kernel 1
(interleaved addressing with    8.054 ms          2.083 GB/s 
divergent branching)

Kernel 2
(interleaved addressing with    3.456 ms          4.854 GB/s   2.33x          2.33x
bank conflicts)

Kernel 3
(sequential addressing)         1.722 ms          9.741 GB/s   2.01x          4.68x

Kernel 4
(first add during               0.965 ms         17.377 GB/s   1.78x          8.34x
global load)
</code></pre><h3 id="Reduction_#5:_Unroll_The_Loop">Reduction #5: Unroll The Loop</h3>
<p>这时我们的数据带宽已经达到了17 GB/s，而我们清楚Reduction的算术强度（arithmetic intensity）很低，因此系统的瓶颈可能是由于Parallel Slowdown，即系统对于指令、调度的花费超过了实际数据处理的花费。在本例中即address arithmetic and loop overhead。</p>
<p>我们的解决办法是将for循环展开（Unroll the loop）。我们知道，在Reduce的过程中，活动的线程数是越来越少的，当活动的线程数少于32个时，我们将只有一个线程束（Warp）。在单个Warp中，指令的执行遵循SIMD（Single Instruction Multiple Data）模式，也就是说在活动线程数少于32个时，我么不需要进行同步控制，即我们不需要 <code>if (tid &lt; s)</code> 。</p>
<pre><code>:::C
/*
    <span class="type">This</span> version unrolls the last warp to avoid synchronization where it
    isn't needed.

    <span class="type">Note</span>, this kernel needs a minimum <span class="keyword">of</span> <span class="number">64</span>*sizeof(T) bytes <span class="keyword">of</span> <span class="literal">shared</span> memory.
    <span class="type">In</span> other words <span class="keyword">if</span> blockSize &lt;= <span class="number">32</span>, allocate <span class="number">64</span>*sizeof(T) bytes.
    <span class="type">If</span> blockSize &gt; <span class="number">32</span>, allocate blockSize*sizeof(T) bytes.
*/
<span class="keyword">template</span> &lt;class T, unsigned <span class="type">int</span> blockSize&gt;
__global__ <span class="type">void</span>
reduce4(T *g_idata, T *g_odata, unsigned <span class="type">int</span> n)
{
    T *sdata = <span class="type">SharedMemory</span>&lt;T&gt;();

    // perform first level <span class="keyword">of</span> reduction,
    // reading <span class="keyword">from</span> global memory, writing to <span class="literal">shared</span> memory
    unsigned <span class="type">int</span> tid = threadIdx.x;
    unsigned <span class="type">int</span> i = blockIdx.x*(blockDim.x*<span class="number">2</span>) + threadIdx.x;

    T mySum = (i &lt; n) ? g_idata[i] : <span class="number">0</span>;

    <span class="keyword">if</span> (i + blockSize &lt; n)
        mySum += g_idata[i+blockSize];

    sdata[tid] = mySum;
    __syncthreads();

    // <span class="keyword">do</span> reduction <span class="keyword">in</span> <span class="literal">shared</span> mem
    <span class="keyword">for</span> (unsigned <span class="type">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">32</span>; s&gt;&gt;=<span class="number">1</span>)
    {
        <span class="keyword">if</span> (tid &lt; s)
        {
            sdata[tid] = mySum = mySum + sdata[tid + s];
        }

        __syncthreads();
    }

    <span class="keyword">if</span> (tid &lt; <span class="number">32</span>)
    {
        // now that we are <span class="keyword">using</span> warp-synchronous programming (below)
        // we need to declare our <span class="literal">shared</span> memory volatile so that the compiler
        // doesn't reorder stores to it <span class="keyword">and</span> induce incorrect behavior.
        volatile T *smem = sdata;

        <span class="keyword">if</span> (blockSize &gt;=  <span class="number">64</span>)
        {
            smem[tid] = mySum = mySum + smem[tid + <span class="number">32</span>];
        }

        <span class="keyword">if</span> (blockSize &gt;=  <span class="number">32</span>)
        {
            smem[tid] = mySum = mySum + smem[tid + <span class="number">16</span>];
        }

        <span class="keyword">if</span> (blockSize &gt;=  <span class="number">16</span>)
        {
            smem[tid] = mySum = mySum + smem[tid +  <span class="number">8</span>];
        }

        <span class="keyword">if</span> (blockSize &gt;=   <span class="number">8</span>)
        {
            smem[tid] = mySum = mySum + smem[tid +  <span class="number">4</span>];
        }

        <span class="keyword">if</span> (blockSize &gt;=   <span class="number">4</span>)
        {
            smem[tid] = mySum = mySum + smem[tid +  <span class="number">2</span>];
        }

        <span class="keyword">if</span> (blockSize &gt;=   <span class="number">2</span>)
        {
            smem[tid] = mySum = mySum + smem[tid +  <span class="number">1</span>];
        }
    }

    // write <span class="literal">result</span> <span class="keyword">for</span> this <span class="keyword">block</span> to global mem
    <span class="keyword">if</span> (tid == <span class="number">0</span>) g_odata[blockIdx.x] = sdata[<span class="number">0</span>];
}
</code></pre><p>注意，这在所有的warps中都省去了无用过的过程，不只是最后一个warp。如果不进行循环展开，则所有的warps都会执行for中的每一次循环和每一次if判断。</p>
<p>本次运行结果如下(4M element)：</p>
<pre><code><span class="attribute">:::BASH</span>    
<span class="code">                                Time(2^22 ints)   Bandwidth    Step Speedup   Culmulative </span>
<span class="header">                                                                              Speedup
-----------------------------------------------------------------------------------------</span>
Kernel 1
(interleaved addressing with    8.054 ms          2.083 GB/s 
divergent branching)

Kernel 2
(interleaved addressing with    3.456 ms          4.854 GB/s   2.33x          2.33x
bank conflicts)

Kernel 3
(sequential addressing)         1.722 ms          9.741 GB/s   2.01x          4.68x

Kernel 4
(first add during               0.965 ms         17.377 GB/s   1.78x          8.34x
global load)

Kernel 5
(unroll last warp)              0.536 ms         31.289 GB/s   1.8x          15.01x
</code></pre><p>今天我们暂时先分析到这里，SDK的示例中还有第六种和第七种优化方案，分别是Completely Unrolled和Multiple Adds / Thread，最后性能提升达30+x，我们以后有机会再仔细进行分析。</p>
<p>为了阅读方便，文末附上CUDA SDK中关于Reduction的算法说明作为参考，本文的内容全部摘录于此。地址：<a href="http://docs.nvidia.com/cuda/samples/6_Advanced/reduction/doc/reduction.pdf" target="_blank" rel="external">点击</a></p>

    </section>

    

<section class="post-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      var disqus_shortname = 'zlmoment'; 
      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


  </article>


            <footer class="footer">
    <span class="footer__copyright">&copy; 2014. All rights reserved.</span>
    <span class="footer__copyright"><a href="http://uno.daleanthony.com" title="link to page for Uno Ghost theme">Uno theme</a> by <a href="http://daleanthony.com" title="link to website for Dale-Anthony">Dale-Anthony</a> | Redesign by <a href="http://letiantian.me" title="">樂天</a></span>
</footer>
        </div>
    </div>

    <!-- The main JavaScript file for Casper -->
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    <script src="/js/jquery.imagesloaded.min.js"></script>
    <script src="/js/gallery.js"></script>
    <script src="/js/jquery.githubRepoWidget.min.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config"> 
      MathJax.Hub.Config({ 
        tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
    }); 
    </script>

    <!--kill ie6 -->
    <!--[if IE 6]>
      <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
    <![endif]-->

    <!--  google analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-37227120-1', 'hackecho.com');
      ga('send', 'pageview');
    </script>

    <!-- totop -->
    <div id="totop" style="position:fixed;bottom:50px;right:30px;cursor: pointer;">
    <a title="back to top"><img style="width:30px;height:30px;" src="/images/totop.png"/></a>
    </div>
    <script>
    (function($) { 
      var upperLimit = 100;
      var scrollElem = $('#totop');
      var scrollSpeed = 500; 
      scrollElem.hide();
      $(window).scroll(function () {            
        var scrollTop = $(document).scrollTop();       
        if ( scrollTop > upperLimit ) {
          $(scrollElem).stop().fadeTo(300, 1); // fade back in           
        }else{       
          $(scrollElem).stop().fadeTo(300, 0); // fade out
        }
      });
      $(scrollElem).click(function(){
        $('html, body').animate({scrollTop:0}, scrollSpeed); return false;
      });
    })(jQuery);
    </script>

    <!-- fancybox -->
    
    <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
    <script src="/fancybox/jquery.fancybox.pack.js"></script>
    <script type="text/javascript">
      (function($){
        $('.fancybox').fancybox();
      })(jQuery);
    </script>
    
    <script src="/js/instantclick.min.js" data-no-instant></script>
    <script data-no-instant>InstantClick.init();</script>
</body>
</html>
