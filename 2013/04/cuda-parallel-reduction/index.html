<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>CUDA中并行规约（Parallel Reduction）的优化 | Hackecho</title>
  <meta name="author" content="Zhaoyu Li">
  
  <meta name="description" content="Parallel Reduction是NVIDIA-CUDA自带的例子，也几乎是所有CUDA学习者的的必看算法。在这个算法的优化中，Mark Harris为我们实现了7种不同的优化版本，将Bandwidth几乎提高到了峰值。相信我们通过仔细研读这个过程，一定能对CUDA程序的优化有更加深刻的认识。下面我们来一一细看这几种优化方案，数据和思想均摘录自官方SDK中Samples的算法说明。
Parallel Reduction
Parallel Reduction可以理解为将一个数组中的所有数相加求和的过程并行化。一般来讲，我们并行化的思路是基于“树”的二元规约，如下图：">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="CUDA中并行规约（Parallel Reduction）的优化"/>
  <meta property="og:site_name" content="Hackecho"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Hackecho" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Hackecho</a></h1>
  <h2><a href="/">Blog by Zhaoyu Li</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div></header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2013-04-25T04:57:00.000Z"><a href="/2013/04/cuda-parallel-reduction/">Apr 24 2013</a></time>
      
      
  
    <h1 class="title">CUDA中并行规约（Parallel Reduction）的优化</h1>
  

    </header>
    <div class="entry">
      
        <p>Parallel Reduction是NVIDIA-CUDA自带的例子，也几乎是所有CUDA学习者的的必看算法。在这个算法的优化中，Mark Harris为我们实现了7种不同的优化版本，将Bandwidth几乎提高到了峰值。相信我们通过仔细研读这个过程，一定能对CUDA程序的优化有更加深刻的认识。下面我们来一一细看这几种优化方案，<strong>数据和思想均摘录自官方SDK中Samples的算法说明</strong>。</p>
<h3 id="parallel-reduction">Parallel Reduction</h3>
<p>Parallel Reduction可以理解为将一个数组中的所有数相加求和的过程并行化。一般来讲，我们并行化的思路是基于“树”的二元规约，如下图：</p>
<p><a href="/images/2013/04/parallel_reduction_tree_based.png"><img src="/images/2013/04/parallel_reduction_tree_based.png" alt=""></a></p>
<a id="more"></a>

<p>但是这样的算法会产生一个问题，就是我们怎样让不同blocks中的线程通信呢？CUDA本身并不支持全局同步（global synchronization）。但是，CUDA的kernel运行时有一个特性，即同一时间只能有一个kernel运行，这样我们便可以将每一层规约作为一个kernel来重复递归调用。如下图：</p>
<p><a href="/images/2013/04/recuisive_tree_based.png"><img src="/images/2013/04/recuisive_tree_based.png" alt=""></a></p>
<p>我们的目标就是基于这个算法进行优化，达到“榨干CUDA性能”的目的。我们选取Bandwidth作为测量标准（因为Bandwidth侧重于测量memory-bound kernels，而GFLOP/s侧重于测量compute-bound kernels）。我们最终目标是实现最大的Data Bandwidth。测试环境为G80 GPU，384-bit memory interface, 900 MHz DDR，Bandwidth峰值384 * 1800 / 8 = 86.4 GB/s。</p>
<p>对于基本概念，放上一张图供参考：</p>
<p><a href="/images/2013/04/base.png"><img src="/images/2013/04/base.png" alt=""></a></p>
<h3 id="reduction-1-interleaved-addressing">Reduction #1: Interleaved Addressing</h3>
<p>Interleaved Addressing的核心思想在于交错寻址，即典型的树状模型。示意图如下：</p>
<p><a href="/images/2013/04/Interleaved_Addressing.png"><img src="/images/2013/04/Interleaved_Addressing.png" alt=""></a></p>
<pre><code>/* This reduction interleaves which threads are active by using the modulo
   operator.  This operator is very expensive on GPUs, and the interleaved
   inactivity means that no whole warps are active, which is also very
   inefficient 
*/
template &lt;class T&gt;
__global__ void
reduce0(T *g_idata, T *g_odata, unsigned int n)
{
    T *sdata = SharedMemory&lt;T&gt;();

    // load shared mem
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;

    sdata[tid] = (i &lt; n) ? g_idata[i] : 0;

    __syncthreads();

    // do reduction in shared mem
    for (unsigned int s=1; s &lt; blockDim.x; s *= 2)
    {
        // modulo arithmetic is slow!
        if ((tid % (2*s)) == 0)
        {
            sdata[tid] += sdata[tid + s];
        }

        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = sdata[0];
}
</code></pre><p><strong>存在的问题：</strong></p>
<p>上述代码中for循环内部，容易出现线程束的分化（Warp Divergence），即同一个Warp中的线程需要执行不同的逻辑分支（详见<a href="http://people.maths.ox.ac.uk/gilesm/cuda/lecs/lec3-2x2.pdf" target="_blank">这里</a>），这是非常低效的，而且 <code>&amp;</code> 运算也是非常慢的。测试结果如下(4M element)：</p>
<pre><code>:::BASH    
                                    Time (2^22 ints)      Bandwidth
--------------------------------------------------------------------------
Kernel 1
(interleaved addressing with        8.054 ms              2.083 GB/s
divergent branching)
</code></pre><p>注意：Block Size = 128 threads for all tests.</p>
<h3 id="reduction-2-interleaved-addressing">Reduction #2: Interleaved Addressing</h3>
<p>为了尽量减少1中的线程束的分化，我们这一步将分化的分支替换为跨步寻址（strided index）：</p>
<pre><code>/* This version uses contiguous threads, but its interleaved
   addressing results in many shared memory bank conflicts.
*/
template &lt;class T&gt;
__global__ void
reduce1(T *g_idata, T *g_odata, unsigned int n)
{
    T *sdata = SharedMemory&lt;T&gt;();

    // load shared mem
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;

    sdata[tid] = (i &lt; n) ? g_idata[i] : 0;

    __syncthreads();

    // do reduction in shared mem
    for (unsigned int s=1; s &lt; blockDim.x; s *= 2)
    {
        int index = 2 * s * tid;

        if (index &lt; blockDim.x)
        {
            sdata[index] += sdata[index + s];
        }

        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = sdata[0];
}
</code></pre><p>示意图如下（注意下图与上图中Thread ID的区别）：</p>
<p><a href="/images/2013/04/Interleaved_Addressing2.png"><img src="/images/2013/04/Interleaved_Addressing2.png" alt=""></a></p>
<p>这里我们遇到一个新的问题，即Shared Memory Bank Conflicts。为了达到高带宽，Shared Memory被划分成许多大小相同的内存块，叫做Banks。Banks可以同步访问，即不同的地址对不同的Banks可以同时读写。但是，如果两个内存请求的地址落到同一个Bank上，将会导致Bank Conflicts，严重影响并行程序的性能。</p>
<p>运行结果如下(4M element)：</p>
<pre><code>:::BASH    
                                Time(2^22 ints)   Bandwidth    Step Speedup   Culmulative
                                                                              Speedup
-----------------------------------------------------------------------------------------
Kernel 1
(interleaved addressing with    8.054 ms          2.083 GB/s 
divergent branching)

Kernel 2
(interleaved addressing with    3.456 ms          4.854 GB/s   2.33x          2.33x
bank conflicts)
</code></pre><h3 id="reduction-3-sequential-addressing">Reduction #3: Sequential Addressing</h3>
<p>我们知道，CUDA中对数据的连续读取效率要比其它方式高。因此我们这一步优化主要是将取址方式变为连续的。我们只需要将2中跨步寻址（strided index）替换为基于threadID的逆向for循环即可。</p>
<pre><code>/*
    This version uses sequential addressing -- no divergence or bank conflicts.
*/
template &lt;class T&gt;
__global__ void
reduce2(T *g_idata, T *g_odata, unsigned int n)
{
    T *sdata = SharedMemory&lt;T&gt;();

    // load shared mem
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;

    sdata[tid] = (i &lt; n) ? g_idata[i] : 0;

    __syncthreads();

    // do reduction in shared mem
    for (unsigned int s=blockDim.x/2; s&gt;0; s&gt;&gt;=1)
    {
        if (tid &lt; s)
        {
            sdata[tid] += sdata[tid + s];
        }

        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = sdata[0];
}
</code></pre><p>示意图如下：</p>
<p><a href="/images/2013/04/Sequential_Addressing.png"><img src="/images/2013/04/Sequential_Addressing.png" alt=""></a></p>
<p>但新的问题又出现了，我们发现在for循环中，因为 <code>if (tid &lt; s)</code> 的缘故，在第一次循环的时候有一半的线程都处于闲置状态！如果我们能全部利用的话，相信性能还会提升很多。这也是我们以后要进行优化的地方，避免线程闲置。</p>
<p>本次运行结果如下(4M element)：</p>
<pre><code>:::BASH    
                                Time(2^22 ints)   Bandwidth    Step Speedup   Culmulative 
                                                                              Speedup
-----------------------------------------------------------------------------------------
Kernel 1
(interleaved addressing with    8.054 ms          2.083 GB/s 
divergent branching)

Kernel 2
(interleaved addressing with    3.456 ms          4.854 GB/s   2.33x          2.33x
bank conflicts)

Kernel 3
(sequential addressing)         1.722 ms          9.741 GB/s   2.01x          4.68x
</code></pre><h3 id="reduction-4-first-add-during-load">Reduction #4: First Add During Load</h3>
<p>在以前的所有版本中，我们都是事先将global的数据读入共享内存 <code>sdata[tid] = (i &lt; n) ? g_idata[i] : 0;</code> ，我们可不可以在这一步进行优化呢？当然，我们这一步优化的目的是在将数据读入到共享内存时同时进行第一次(第一层)规约。</p>
<pre><code>:::C
/*
    This version uses n/2 threads --
    it performs the first level of reduction when reading from global memory.
*/
template &lt;class T&gt;
__global__ void
reduce3(T *g_idata, T *g_odata, unsigned int n)
{
    T *sdata = SharedMemory&lt;T&gt;();

    // perform first level of reduction,
    // reading from global memory, writing to shared memory
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x*(blockDim.x*2) + threadIdx.x;

    T mySum = (i &lt; n) ? g_idata[i] : 0;

    if (i + blockDim.x &lt; n)
        mySum += g_idata[i+blockDim.x];

    sdata[tid] = mySum;
    __syncthreads();

    // do reduction in shared mem
    for (unsigned int s=blockDim.x/2; s&gt;0; s&gt;&gt;=1)
    {
        if (tid &lt; s)
        {
            sdata[tid] = mySum = mySum + sdata[tid + s];
        }

        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = sdata[0];
}
</code></pre><p>本次运行结果如下(4M element)：</p>
<pre><code>:::BASH    
                                Time(2^22 ints)   Bandwidth    Step Speedup   Culmulative 
                                                                              Speedup
-----------------------------------------------------------------------------------------
Kernel 1
(interleaved addressing with    8.054 ms          2.083 GB/s 
divergent branching)

Kernel 2
(interleaved addressing with    3.456 ms          4.854 GB/s   2.33x          2.33x
bank conflicts)

Kernel 3
(sequential addressing)         1.722 ms          9.741 GB/s   2.01x          4.68x

Kernel 4
(first add during               0.965 ms         17.377 GB/s   1.78x          8.34x
global load)
</code></pre><h3 id="reduction-5-unroll-the-loop">Reduction #5: Unroll The Loop</h3>
<p>这时我们的数据带宽已经达到了17 GB/s，而我们清楚Reduction的算术强度（arithmetic intensity）很低，因此系统的瓶颈可能是由于Parallel Slowdown，即系统对于指令、调度的花费超过了实际数据处理的花费。在本例中即address arithmetic and loop overhead。</p>
<p>我们的解决办法是将for循环展开（Unroll the loop）。我们知道，在Reduce的过程中，活动的线程数是越来越少的，当活动的线程数少于32个时，我们将只有一个线程束（Warp）。在单个Warp中，指令的执行遵循SIMD（Single Instruction Multiple Data）模式，也就是说在活动线程数少于32个时，我么不需要进行同步控制，即我们不需要 <code>if (tid &lt; s)</code> 。</p>
<pre><code>:::C
/*
    This version unrolls the last warp to avoid synchronization where it
    isn&#39;t needed.

    Note, this kernel needs a minimum of 64*sizeof(T) bytes of shared memory.
    In other words if blockSize &lt;= 32, allocate 64*sizeof(T) bytes.
    If blockSize &gt; 32, allocate blockSize*sizeof(T) bytes.
*/
template &lt;class T, unsigned int blockSize&gt;
__global__ void
reduce4(T *g_idata, T *g_odata, unsigned int n)
{
    T *sdata = SharedMemory&lt;T&gt;();

    // perform first level of reduction,
    // reading from global memory, writing to shared memory
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x*(blockDim.x*2) + threadIdx.x;

    T mySum = (i &lt; n) ? g_idata[i] : 0;

    if (i + blockSize &lt; n)
        mySum += g_idata[i+blockSize];

    sdata[tid] = mySum;
    __syncthreads();

    // do reduction in shared mem
    for (unsigned int s=blockDim.x/2; s&gt;32; s&gt;&gt;=1)
    {
        if (tid &lt; s)
        {
            sdata[tid] = mySum = mySum + sdata[tid + s];
        }

        __syncthreads();
    }

    if (tid &lt; 32)
    {
        // now that we are using warp-synchronous programming (below)
        // we need to declare our shared memory volatile so that the compiler
        // doesn&#39;t reorder stores to it and induce incorrect behavior.
        volatile T *smem = sdata;

        if (blockSize &gt;=  64)
        {
            smem[tid] = mySum = mySum + smem[tid + 32];
        }

        if (blockSize &gt;=  32)
        {
            smem[tid] = mySum = mySum + smem[tid + 16];
        }

        if (blockSize &gt;=  16)
        {
            smem[tid] = mySum = mySum + smem[tid +  8];
        }

        if (blockSize &gt;=   8)
        {
            smem[tid] = mySum = mySum + smem[tid +  4];
        }

        if (blockSize &gt;=   4)
        {
            smem[tid] = mySum = mySum + smem[tid +  2];
        }

        if (blockSize &gt;=   2)
        {
            smem[tid] = mySum = mySum + smem[tid +  1];
        }
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = sdata[0];
}
</code></pre><p>注意，这在所有的warps中都省去了无用过的过程，不只是最后一个warp。如果不进行循环展开，则所有的warps都会执行for中的每一次循环和每一次if判断。</p>
<p>本次运行结果如下(4M element)：</p>
<pre><code>:::BASH    
                                Time(2^22 ints)   Bandwidth    Step Speedup   Culmulative 
                                                                              Speedup
-----------------------------------------------------------------------------------------
Kernel 1
(interleaved addressing with    8.054 ms          2.083 GB/s 
divergent branching)

Kernel 2
(interleaved addressing with    3.456 ms          4.854 GB/s   2.33x          2.33x
bank conflicts)

Kernel 3
(sequential addressing)         1.722 ms          9.741 GB/s   2.01x          4.68x

Kernel 4
(first add during               0.965 ms         17.377 GB/s   1.78x          8.34x
global load)

Kernel 5
(unroll last warp)              0.536 ms         31.289 GB/s   1.8x          15.01x
</code></pre><p>今天我们暂时先分析到这里，SDK的示例中还有第六种和第七种优化方案，分别是Completely Unrolled和Multiple Adds / Thread，最后性能提升达30+x，我们以后有机会再仔细进行分析。</p>
<p>为了阅读方便，文末附上CUDA SDK中关于Reduction的算法说明作为参考，本文的内容全部摘录于此。地址：<a href="http://docs.nvidia.com/cuda/samples/6_Advanced/reduction/doc/reduction.pdf" target="_blank">点击</a></p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/cuda,parallel,reduction/">cuda,parallel,reduction</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:hackecho.com">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Recent Posts</h3>
  <ul class="entry">
    
      <li>
        <a href="/2014/04/hadoop-cluster-setup-instruction-with-fedora-20/">Hadoop Cluster Setup Instruction with Fedora 20</a>
      </li>
    
      <li>
        <a href="/2013/10/real-time-emotion-analysis-on-twitter/">Real-time Emotion Analysis On Twitter</a>
      </li>
    
      <li>
        <a href="/2013/04/cuda-parallel-reduction/">CUDA中并行规约（Parallel Reduction）的优化</a>
      </li>
    
      <li>
        <a href="/2013/04/basics-of-mpi/">MPI 并行程序设计基础</a>
      </li>
    
      <li>
        <a href="/2013/03/a-letter-to-myself/">写给四年前刚开始编程的自己</a>
      </li>
    
  </ul>
</div>


  <div class="widget tag">
<h3 class="title">Links</h3>
<ul class="entry">
<li><a href="http://vicdory.com/" title="Kailun Shi">Kailun Shi</a></li>
</ul>
</div>

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/Android/">Android</a><small>1</small></li>
  
    <li><a href="/tags/Android,AsyncTask/">Android,AsyncTask</a><small>1</small></li>
  
    <li><a href="/tags/Android,SharedPreferences/">Android,SharedPreferences</a><small>1</small></li>
  
    <li><a href="/tags/BidData/">BidData</a><small>1</small></li>
  
    <li><a href="/tags/Decorator,Python/">Decorator,Python</a><small>1</small></li>
  
    <li><a href="/tags/GBK,Mac,UTF8/">GBK,Mac,UTF8</a><small>1</small></li>
  
    <li><a href="/tags/Git/">Git</a><small>1</small></li>
  
    <li><a href="/tags/Google+/">Google+</a><small>1</small></li>
  
    <li><a href="/tags/Hadoop/">Hadoop</a><small>1</small></li>
  
    <li><a href="/tags/JSON,XML,PHP/">JSON,XML,PHP</a><small>1</small></li>
  
    <li><a href="/tags/Life/">Life</a><small>1</small></li>
  
    <li><a href="/tags/Linux,HFS/">Linux,HFS</a><small>1</small></li>
  
    <li><a href="/tags/MPI/">MPI</a><small>1</small></li>
  
    <li><a href="/tags/MVC, PHP/">MVC, PHP</a><small>1</small></li>
  
    <li><a href="/tags/PHP,Socket/">PHP,Socket</a><small>1</small></li>
  
    <li><a href="/tags/Python/">Python</a><small>1</small></li>
  
    <li><a href="/tags/ThinkPHP/">ThinkPHP</a><small>1</small></li>
  
    <li><a href="/tags/WebSocket,HTML5/">WebSocket,HTML5</a><small>1</small></li>
  
    <li><a href="/tags/Wordpress/">Wordpress</a><small>2</small></li>
  
    <li><a href="/tags/boost,thread/">boost,thread</a><small>1</small></li>
  
    <li><a href="/tags/cuda,parallel,reduction/">cuda,parallel,reduction</a><small>1</small></li>
  
    <li><a href="/tags/fly-of-promgrammer/">fly-of-promgrammer</a><small>1</small></li>
  
    <li><a href="/tags/gcc/">gcc</a><small>1</small></li>
  
    <li><a href="/tags/ipc,semaphores/">ipc,semaphores</a><small>1</small></li>
  
    <li><a href="/tags/life/">life</a><small>1</small></li>
  
    <li><a href="/tags/makefile/">makefile</a><small>1</small></li>
  
    <li><a href="/tags/movie/">movie</a><small>1</small></li>
  
    <li><a href="/tags/open source/">open source</a><small>1</small></li>
  
    <li><a href="/tags/pthread/">pthread</a><small>1</small></li>
  
    <li><a href="/tags/web/">web</a><small>1</small></li>
  
    <li><a href="/tags/websocket/">websocket</a><small>1</small></li>
  
    <li><a href="/tags/极客/">极客</a><small>1</small></li>
  
    <li><a href="/tags/程序员/">程序员</a><small>2</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2014 Zhaoyu Li
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'zlmoment';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>